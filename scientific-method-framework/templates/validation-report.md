# Validation Report - [Investigation Name]

**Investigation ID**: [Auto-generated or manual ID]
**Created**: [YYYY-MM-DD HH:MM:SS UTC]
**Completed**: [YYYY-MM-DD HH:MM:SS UTC or "Ongoing"]
**Investigator**: [Name]
**Status**: [In Progress/Completed/Validated/Inconclusive]

## Executive Summary

**Primary Conclusion**: [Brief statement of main finding]
**Confidence Level**: [XX% with justification]
**Recommendation**: [Specific action items based on findings]
**Business Impact**: [Quantified impact on users, systems, business metrics]

## Investigation Metrics

### Scope and Duration
- **Duration**: [Start time] → [End time] ([Total hours/days])
- **Hypotheses Registered**: [Count]
- **Hypotheses Tested**: [Count with success rate]
- **Evidence Sources**: [Count by type with credibility distribution]
- **False Leads**: [Count with lessons learned]
- **Experiments Conducted**: [Count with outcomes]

### Quality Indicators
- **Evidence Diversity**: [X source types across Y categories]
- **Multi-source Validation**: [% of conclusions supported by ≥3 sources]
- **Alternative Testing**: [% of investigations that tested competing hypotheses]
- **Validation Rate**: [% of conclusions independently verified]

## Hypothesis Analysis

### Primary Hypothesis: [ID and Description]
**Status**: [Validated/Rejected/Inconclusive]
**Confidence**: [XX% with statistical backing]

**Supporting Evidence**:
- [High-credibility evidence with source and credibility score]
- [Additional supporting evidence]
- [Statistical measurements and confidence intervals]

**Contradicting Evidence**:
- [Evidence that challenges this hypothesis]
- [Limitations or uncertainties]
- [Alternative explanations for supporting evidence]

**Validation Method**: [How hypothesis was tested]
**Statistical Significance**: [p-value, confidence intervals, effect sizes]

### Alternative Hypotheses
#### Hypothesis [ID]: [Description]
**Status**: [Rejected/Insufficient Evidence/Requires Further Investigation]
**Confidence**: [XX%]
**Key Findings**: [Summary of testing results]

## Evidence Summary

### Evidence Quality Assessment
| Evidence Category | Count | Avg Credibility | Weight in Conclusion |
|------------------|-------|-----------------|---------------------|
| Direct Measurement | [N] | [X.X/10] | [XX%] |
| Structured Logs | [N] | [X.X/10] | [XX%] |
| Code Analysis | [N] | [X.X/10] | [XX%] |
| User Reports | [N] | [X.X/10] | [XX%] |
| Other | [N] | [X.X/10] | [XX%] |

### Statistical Validation
- **Total Evidence Points**: [Weighted sum of evidence scores]
- **Credibility Weighted Score**: [X.X/10]
- **Evidence Diversity Index**: [X source types utilized]
- **Independent Validation Rate**: [XX% of findings independently verified]

## Confidence Assessment

### Investigation Confidence Score: [X.X/10]
- **Hypothesis Support**: [X/10] ([Weight %])
- **Evidence Quality**: [X/10] ([Weight %])
- **Evidence Diversity**: [X/10] ([Weight %])
- **Alternative Testing**: [X/10] ([Weight %])

### Confidence Interpretation
**[X.X/10]**: [High/Good/Moderate/Low confidence interpretation]
**Recommended Action**: [Based on confidence level]
- High confidence (8.5+): Proceed with production implementation
- Good confidence (7.0-8.4): Validate in staging environment first
- Moderate confidence (5.5-6.9): Additional investigation recommended
- Low confidence (<5.5): Significant additional investigation required

### Uncertainty Quantification
**Known Uncertainties**:
- [Specific areas where information is incomplete]
- [Confidence intervals around key measurements]
- [Assumptions that could not be validated]

**Risk Assessment**:
- [Probability and impact of being wrong]
- [Potential consequences of incorrect conclusion]
- [Mitigation strategies for key risks]

## Experimental Validation Results

### Experiments Conducted
| Experiment | Type | Status | Primary Result | Statistical Significance |
|------------|------|--------|---------------|------------------------|
| [EXP-001] | [A/B Test] | [Complete] | [Hypothesis validated] | [p < 0.01, CI: X-Y] |
| [EXP-002] | [Staging] | [Complete] | [Performance improved] | [Effect size: large] |

### Key Experimental Findings
**[Experiment Name]**: [Brief description and results]
- **Outcome**: [Success/Failure/Inconclusive]
- **Statistical Significance**: [p-value, confidence intervals]
- **Effect Size**: [Practical significance assessment]
- **Business Impact**: [Quantified improvement or impact]

## Reproducibility Assessment

### Reproducibility Checklist
- [ ] Investigation steps documented with precise timestamps
- [ ] Data sources and queries preserved for replication
- [ ] Statistical methods and assumptions clearly specified
- [ ] Independent validation possible with provided documentation
- [ ] False leads and dead ends documented for learning

### Replication Instructions
**To Reproduce Main Findings**:
1. [Step-by-step instructions for reproducing key evidence]
2. [Required data sources and access methods]
3. [Analysis scripts and statistical methods used]
4. [Expected results and acceptable variance ranges]

### Data Preservation
**Preserved Artifacts**:
- [Links to raw data files, database queries, log exports]
- [Analysis scripts and configuration files]
- [Screenshots and dashboard exports]
- [Communication records and decision rationale]

## Bias Assessment

### Bias Mitigation Review
**Confirmation Bias Check**: [✓/✗ with explanation]
- [Evidence of alternative hypothesis testing]
- [Documentation of contradictory evidence]
- [Independent review or validation]

**Selection Bias Check**: [✓/✗ with explanation]
- [Representative sampling methods used]
- [Potential sampling biases identified and addressed]

**Temporal Bias Check**: [✓/✗ with explanation]
- [Both recent and historical evidence considered]
- [Seasonal or cyclical factors accounted for]

**Source Bias Check**: [✓/✗ with explanation]
- [Multiple independent evidence sources utilized]
- [Potential source biases identified and weighted appropriately]

### Calibration Assessment
**Historical Accuracy**: [XX% of similar confidence predictions were correct]
**Calibration Trend**: [Improving/Stable/Needs Work]
**Adjustment Recommendations**: [How to improve future calibration]

## Business Impact Analysis

### Quantified Impacts
**User Impact**:
- Affected users: [X,XXX users (CI: X,XXX - X,XXX)]
- Impact duration: [X hours/days]
- User experience degradation: [Quantified metrics]

**System Impact**:
- Performance impact: [Response time, error rate changes]
- Resource utilization: [CPU, memory, database impacts]
- Availability impact: [Uptime, SLA metrics]

**Business Impact**:
- Revenue impact: [$ amount if quantifiable]
- Customer satisfaction: [Support ticket volume, NPS impact]
- Operational overhead: [Engineering time, support load]

## Recommendations

### Immediate Actions
1. **[High Priority]**: [Specific action with timeline]
   - **Rationale**: [Why this action is recommended]
   - **Expected Outcome**: [Predicted result]
   - **Success Metrics**: [How to measure success]

2. **[Medium Priority]**: [Second priority action]
   - **Rationale**: [Justification]
   - **Dependencies**: [What must happen first]

### Long-term Improvements
**System Improvements**: [Architectural or process changes to prevent recurrence]
**Monitoring Enhancements**: [Additional monitoring to detect similar issues]
**Process Improvements**: [Changes to investigation or development processes]

### Follow-up Investigations
**Additional Questions**: [Areas requiring further investigation]
**Monitoring Plan**: [How to validate solution effectiveness]
**Learning Opportunities**: [Knowledge to be captured and shared]

## Quality Assurance Validation

### Scientific Rigor Checklist
- [ ] Multiple competing hypotheses considered and tested
- [ ] Evidence from diverse, independent sources
- [ ] Statistical significance testing performed where appropriate
- [ ] Confidence levels calibrated against historical accuracy
- [ ] Bias mitigation strategies employed
- [ ] Reproducibility documentation complete

### Peer Review Status
**Reviewer**: [Name and role]
**Review Date**: [Date]
**Review Status**: [Approved/Needs Revision/Major Concerns]
**Key Feedback**: [Summary of reviewer comments]

## Cross-References

### Related Documentation
- **Hypothesis Registry**: [Link to hypothesis-registry.md]
- **Evidence Matrix**: [Link to evidence-matrix.md]
- **Experimental Plans**: [Links to experimental-plan.md files]
- **Support Tickets**: [Links to related tickets]
- **Code Changes**: [Links to relevant PRs or commits]

### Integration Handoffs
**Testing Integration**: [How findings integrate with test development]
**Operations Handoff**: [Monitoring and operational considerations]
**Knowledge Sharing**: [Team communication and documentation updates]

---

## Usage Instructions

**Template Usage**:
1. Copy this template to investigation directory
2. Update throughout investigation process
3. Complete comprehensive summary at conclusion
4. Use for investigation post-mortems and learning

**Integration Commands**:
```bash
# Generate validation report automatically
source ~/.claude/skills/scientific-method-framework/scripts/statistical-validation.sh
calculate_investigation_confidence 8 9 4 2

# Track calibration for continuous improvement
track_calibration "ZYN-10585" 85 1  # investigation_id, predicted_confidence, actual_outcome
```

**Quality Standards**:
- Confidence score ≥7.0 for production recommendations
- Minimum 3 independent evidence sources for critical conclusions
- Statistical significance testing for quantitative claims
- Bias assessment and mitigation documentation
- Reproducibility validation complete